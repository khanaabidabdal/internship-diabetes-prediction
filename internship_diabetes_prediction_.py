# -*- coding: utf-8 -*-
"""Internship Diabetes prediction .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10N394y7yRiDv78nikwv46OkqvBP--7T6

The following project is an internship task assign by Meriskill to train a model to predict the diabetes of the women as this dataset contains only a data of women. Let's begin by importing required basic libraries :
"""

#Importing Requires Libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""## Data Analysis & Visualization :"""

raw_df=pd.read_csv('diabetes.csv')

raw_df.head(10)

raw_df.sample(10)

raw_df.sample(10)

raw_df.columns

#Checking Null values
raw_df.isna().sum()

#Checking Datatypes
raw_df.info()

#Checking Unique Values In Each Columns
raw_df.nunique()

#Checking Range of Age
raw_df['Age'].max(),raw_df['Age'].min()

#Checking Statisticals Values
raw_df.describe()

# Commented out IPython magic to ensure Python compatibility.
#Importing Plotly & Setting Grid
import plotly.express as px
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

sns.set_style('darkgrid')
matplotlib.rcParams['font.size'] = 14
matplotlib.rcParams['figure.figsize'] = (10, 6)
matplotlib.rcParams['figure.facecolor'] = '#00000000'

px.histogram(raw_df,x="Age",title='Age Vs Diabetes',color='Outcome')

"""From the above plot, we can draw a conclusion that with increase in age the diabetes also increases. Since,from age 21 to 30 we have more data and less number of diabetes, but after 30 age we have less data and more diabetes."""

sns.countplot(x='Pregnancies',hue="Outcome",data=raw_df);

"""Here, from the bar plot the distribution of diabetes with pregnancies is not showing any trends or some trends which shows that with increase in pregnancies diabetes also increases is may be due to the factor of age because to have more childs the age must also be considerable as a general rule. Therefore,To verify above theory,let's plot some realtionships between Pregnancies,Age and diabetes."""

px.histogram(raw_df,x='Age',title='Age & Pregnancies',color='Pregnancies')

"""Performing an experiment,unselect all the pregnancies label see the realtion between pregnancies and age and you will find that our above theory is right."""

raw_df['Outcome']=raw_df['Outcome'].astype(bool)
px.scatter(raw_df,x="Age",y='Pregnancies',color='Outcome')

"""From above scatter plot, It becomes clear that number of pregnancies is not effecting the diabetes by considerable amount. Here, the trends we observe in the bar plots are due to the age and are not because of number of pregnancies."""

px.histogram(raw_df,x="BMI",title='BMI Vs Diabetes',color='Outcome')

"""From the above plot, we can draw a conclusion that with increase in BMI, the diabetes also increases. Since, after 35 BMI value we have more diabatic women and less data as compared to the before 35 BMI."""

px.histogram(raw_df,x="DiabetesPedigreeFunction",title='DiabetesPedigreeFunction Vs Diabetes',color='Outcome')

"""The above plot clearly shows that with increase in Diabetes Pedigree Function, diabetes also increases. This conclusion also match with the Biological Statement that diabetes is a genetic disease."""

px.scatter(raw_df,x="Glucose",y='Insulin',color='Outcome',title="Insulin & Glucose Relation with Diabetes")

"""From above observation,We can conclude that with high glucose level diabetes increases and with high insulin level diabetes Decreases. In other words, Insulin is inversely propotional to Diabetes while Glucose is durectly proportional to the diabetes."""

px.histogram(raw_df,x="BloodPressure",title='Blood Pressure Vs Diabetes',color='Outcome')

"""From above plot, we can say that Blood Pressure is weakly related to a diabetes with high blood pressure more diabetes."""

px.scatter(raw_df,x="Age",y='BloodPressure',color='Outcome')

"""Here, from scatter plot we can say that here the age is not the fact as like in case of pregnancies."""

px.histogram(raw_df,x="SkinThickness",title='Skin Thickness Vs Diabetes',color='Outcome')

"""From above plot, we can say that thickness is weakly related to a diabetes more with thicker skin more diabetes. But it also takes long duratuion for diabetes to make skin thicker.Hence, we can avoid this in our training of model.

##Model Preparation :
"""

#Importing train_test_split
from sklearn.model_selection import train_test_split

#Spliting The Data :
train_df,test_df=train_test_split(raw_df,test_size=0.2,random_state=1)

print(f"Train_df.shape : {train_df.shape}")
print(f"Test_df.shape : {test_df.shape}")

#Reconverting Outcome to int64
raw_df['Outcome']=raw_df['Outcome'].astype(int)

raw_df.columns

#Identifying Inputs & Target :
input_cols=['Glucose', 'BloodPressure', 'SkinThickness',
            'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']
target_cols='Outcome'

train_inputs=train_df[input_cols].copy()
train_targets=train_df[target_cols].copy()

test_inputs=test_df[input_cols].copy()
test_targets=test_df[target_cols].copy()

"""**Scalling :** It helps to avoid mathematical loss."""

from sklearn.preprocessing import MinMaxScaler

scaler=MinMaxScaler()

numeric_cols = train_inputs.select_dtypes(include=np.number).columns.tolist()

scaler.fit(raw_df[numeric_cols])

train_inputs[numeric_cols]=scaler.transform(train_inputs[numeric_cols])
test_inputs[numeric_cols]=scaler.transform(test_inputs[numeric_cols])

"""###Logistic Regression :"""

#Logistic Regression :
from sklearn.linear_model import LogisticRegression

log_model=LogisticRegression()

log_model.fit(train_inputs,train_targets)

X_train=train_inputs
X_test=test_inputs

log_train_pred=log_model.predict(X_train)
log_train_pred

log_train_pred

log_train_probs=log_model.predict_proba(X_train)
log_train_probs

log_model.classes_

from sklearn.metrics import accuracy_score

accuracy_score(train_targets,log_train_pred)

#Test
log_test_pred=log_model.predict(X_test)
log_test_pred

accuracy_score(test_targets,log_test_pred)

log_train_acc,log_test_acc=accuracy_score(train_targets,log_train_pred),accuracy_score(test_targets,log_test_pred)

"""###Random Forest :"""

#Random Forest
from sklearn.ensemble import RandomForestClassifier

For_model=RandomForestClassifier(n_jobs=-1,random_state=1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# For_model.fit(X_train,train_targets)

For_model.score(X_train,train_targets)

For_model.score(X_test,test_targets)

For_model=RandomForestClassifier(n_jobs=-1,random_state=1,n_estimators=500,max_depth=15,max_leaf_nodes=50)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# For_model.fit(X_train,train_targets)

For_model.score(X_train,train_targets),For_model.score(X_test,test_targets)

For_model=RandomForestClassifier(n_jobs=-1,random_state=1,n_estimators=50,max_depth=5,max_leaf_nodes=50,)

For_model.fit(X_train,train_targets)

For_model.score(X_train,train_targets),For_model.score(X_test,test_targets)

for_train_acc,for_test_acc=For_model.score(X_train,train_targets),For_model.score(X_test,test_targets)

importance_df = pd.DataFrame({
    'feature': X_test.columns,
    'importance': For_model.feature_importances_
}).sort_values('importance', ascending=False)

import seaborn as sns
plt.figure(figsize=(10,6))
plt.title('Feature Importance')
sns.barplot(data=importance_df, x='importance', y='feature');

"""###Gradient Boosting :"""

#XG Boost Model
from xgboost import XGBClassifier

XG_model=XGBClassifier(random_state=42,n_jobs=-1)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# XG_model.fit(X_train,train_targets)

xg_pred=XG_model.predict(X_train)

accuracy_score(train_targets,xg_pred)

xg_test_pred=XG_model.predict(X_test)

accuracy_score(test_targets,xg_test_pred)

xg_model=XGBClassifier(n_jobs=-1,random_state=42,learning_rate=0.3,max_depth=5,max_leaves=5,n_estimators=20,max_bin=10, min_child_weight=10,booster='gbtree')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# xg_model.fit(X_train,train_targets)

xg_train_pred=xg_model.predict(X_train)

xg_test_pred=xg_model.predict(X_test)

accuracy_score(train_targets,xg_train_pred),accuracy_score(test_targets,xg_test_pred)

xg_model=XGBClassifier(n_jobs=-1,random_state=42,learning_rate=1,max_depth=6,max_leaves=6,n_estimators=10,max_bin=50, min_child_weight=10,booster='gbtree')
xg_model.fit(X_train,train_targets)
xg_train_pred=xg_model.predict(X_train)
xg_test_pred=xg_model.predict(X_test)
accuracy_score(train_targets,xg_train_pred),accuracy_score(test_targets,xg_test_pred)

xg_train_acc,xg_test_acc=accuracy_score(train_targets,xg_train_pred),accuracy_score(test_targets,xg_test_pred)

importance_df = pd.DataFrame({
    'feature': X_test.columns,
    'importance': xg_model.feature_importances_
}).sort_values('importance', ascending=False)

import seaborn as sns
plt.figure(figsize=(10,6))
plt.title('Feature Importance')
sns.barplot(data=importance_df, x='importance', y='feature');

"""###Comparison Between Models & Selecting Best One :"""

comparison=pd.DataFrame({
    "Model Name":["Logistic Regression",'Random Forest','Gradient Boosting'],
    "Training Score":[log_train_acc,for_train_acc,xg_train_acc],
    "Test Score":[log_test_acc,for_test_acc,xg_test_acc]
})
comparison

"""From the comaprison it becomes clear that Gradient Boosting is the best model trained on this data.Therefore, we select this model as our final model."""

model=XGBClassifier(n_jobs=-1,random_state=42,learning_rate=1,max_depth=6,max_leaves=6,n_estimators=10,max_bin=50, min_child_weight=10,booster='gbtree')
model.fit(X_train,train_targets)
train_pred=xg_model.predict(X_train)
test_pred=xg_model.predict(X_test)
accuracy_score(train_targets,xg_train_pred),accuracy_score(test_targets,xg_test_pred)

"""##For Custom Data :"""

raw_df.head(2)

def predict_input(model, single_input):
    input_df = pd.DataFrame([single_input])
    input_df[numeric_cols] = scaler.transform(input_df[numeric_cols])
    X_input = input_df[numeric_cols]
    pred = model.predict(X_input)[0]
    prob = model.predict_proba(X_input)[0][list(model.classes_).index(pred)]
    return pred, prob

new_input={'Pregnancies':2,
      'Glucose':200,
      'BloodPressure':100,
      'SkinThickness':30,
      'Insulin':30,
      'BMI':35,
      'DiabetesPedigreeFunction':0.65,
      'Age':50
}

predict_input(model,new_input)

"""From above we can say that our model is working on custom and it is prediting Diabetes as True and is 80% confident about it."""

new_input={'Pregnancies':2,
      'Glucose':100,
      'BloodPressure':100,
      'SkinThickness':30,
      'Insulin':30,
      'BMI':35,
      'DiabetesPedigreeFunction':0.65,
      'Age':20
}
predict_input(model,new_input)

"""Now, With decrease in glucose level and Age our model is predicting No diabetes and is 97% sure about it.

##Conclusion :

The following are the conclusion :

1. Dataset is small that's why model is overfitting in first attempt.
2. The Dataset can include data like diet, difficulty in walking,education about diabetes,etc.
3. Our Moadel is giving 81% accuracy that is good in such problems beacuse the bilogical law are not perfect.
4. With increase in dataset our model can reach accurcy of 90%.
"""